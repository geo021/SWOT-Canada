{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sfoucher/SWOT-Canada/blob/main/Workshops/SCT2024/Workshop_06102024_Halifax_SWOTHR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b11PMt8r3rHd"
      },
      "source": [
        "![](https://img.shields.io/badge/PO.DAAC-Contribution-%20?color=grey&labelColor=blue)\n",
        "\n",
        "> From the PO.DAAC Cookbook, to access the GitHub version of the notebook, follow [this link](https://github.com/podaac/tutorials/blob/master/notebooks/datasets/SWOTHR_localmachine.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHNpTEGt3rHe"
      },
      "source": [
        "# SWOT Hydrology Dataset Exploration on a local machine\n",
        "\n",
        "## Accessing and Visualizing SWOT Datasets\n",
        "\n",
        "### Requirement:\n",
        "Local compute environment e.g. laptop, server: this tutorial can be run on your local machine.\n",
        "\n",
        "### Learning Objectives:\n",
        "- Access SWOT HR data prodcuts (archived in NASA Earthdata Cloud) within the AWS cloud, by downloading to local machine\n",
        "- Visualize accessed data for a quick check\n",
        "\n",
        "#### SWOT Level 2 KaRIn High Rate Version 2.0 Datasets:\n",
        "\n",
        "1. **River Vector Shapefile** - SWOT_L2_HR_RIVERSP_2.0\n",
        "\n",
        "2. **Lake Vector Shapefile** - SWOT_L2_HR_LAKESP_2.0\n",
        "\n",
        "3. **Water Mask Pixel Cloud NetCDF** - SWOT_L2_HR_PIXC_2.0\n",
        "\n",
        "4. **Water Mask Pixel Cloud Vector Attribute NetCDF** - SWOT_L2_HR_PIXCVec_2.0\n",
        "\n",
        "5. **Raster NetCDF** - SWOT_L2_HR_Raster_2.0\n",
        "\n",
        "6. **Single Look Complex Data product** - SWOT_L1B_HR_SLC_2.0\n",
        "\n",
        "_This notebook has been slightly modified by the University of Sherbrooke and University Laval team to run smoothly in Google Colab for the June 10, 2024 training session. Original authors :  Cassie Nickles, NASA PO.DAAC (Feb 2024) || Other Contributors: Zoe Walschots (PO.DAAC Summer Intern 2023), Catalina Taglialatela (NASA PO.DAAC), Luis Lopez (NASA NSIDC DAAC)_\n",
        "\n",
        "_Last updated: 7 juin 2024_\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60WhXudr3rHe"
      },
      "source": [
        "### Libraries Needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iUtXlwn3rHe"
      },
      "outputs": [],
      "source": [
        "!pip install contextily\n",
        "!pip install earthaccess\n",
        "!pip install --upgrade holoviews hvplot\n",
        "!pip install holoviews hvplot bokeh xarray\n",
        "!pip install rioxarray\n",
        "!pip install rasterio\n",
        "!pip install shapely\n",
        "!pip install geoviews\n",
        "#!pip install hvplot\n",
        "\n",
        "\n",
        "import glob\n",
        "import h5netcdf\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import contextily as cx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import hvplot.xarray\n",
        "import holoviews as hv\n",
        "import zipfile\n",
        "import earthaccess\n",
        "import os\n",
        "import rioxarray\n",
        "from shapely.geometry import mapping\n",
        "import csv\n",
        "import shapefile\n",
        "import geoviews as gvts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUjA7H1Y3rHf"
      },
      "source": [
        "### Earthdata Login\n",
        "\n",
        "An Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. If you don't already have one, please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up. We use `earthaccess` to authenticate your login credentials below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqbEXA7x3rHf"
      },
      "outputs": [],
      "source": [
        "auth = earthaccess.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l4soPmS-Jbd"
      },
      "source": [
        "### Single File Access"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEcoud9p3rHf"
      },
      "source": [
        "\n",
        "#### **1. River Vector Shapefiles**\n",
        "\n",
        "The https access link can be found using `earthaccess` data search. Since this collection consists of Reach and Node files, we need to extract only the granule for the Reach file. We do this by filtering for the 'Reach' title in the data link.\n",
        "\n",
        "Alternatively, Earthdata Search [(see tutorial)](https://nasa-openscapes.github.io/2021-Cloud-Workshop-AGU/tutorials/01_Earthdata_Search.html) can be used to manually search in a GUI interface.\n",
        "\n",
        "For additional tips on spatial searching of SWOT HR L2 data, see also [PO.DAAC Cookbook - SWOT Chapter tips section](https://podaac.github.io/tutorials/quarto_text/SWOT.html#tips-for-swot-hr-spatial-search).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RoIlpJpigZ-"
      },
      "source": [
        "#### Search for the data of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hwp1-ZHV3rHg"
      },
      "outputs": [],
      "source": [
        "#Retrieves granule from the day we want, in this case by passing to `earthdata.search_data` function the data collection shortname, temporal bounds, and filter by wildcards\n",
        "river_results = earthaccess.search_data(short_name = 'SWOT_L2_HR_RIVERSP_2.0',\n",
        "                                        #temporal = ('2024-02-01 00:00:00', '2024-02-29 23:59:59'), # can also specify by time\n",
        "                                        granule_name = '*Node*_492_NA*') # here we filter by Nodes files (not reachs), by pass=492 (above Nova Scotia) and by continent code=NA\n",
        "                                                                         # change 'Node' by 'Reach' if you want to extract them instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NP4AgA4x0KAK"
      },
      "outputs": [],
      "source": [
        "#Print the granule characteristics associated with the selected pass.\n",
        "print(river_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGGZ2Jxu3rHg"
      },
      "source": [
        "#### Dowload, unzip, read the data\n",
        "\n",
        "Let's download the selected data file! `earthaccess.download` has a list as the input format, so we need to put brackets around the single file we pass.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz2o8DL83rHg"
      },
      "outputs": [],
      "source": [
        "earthaccess.download([river_results[3]], \"./data_downloads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr6Tvrdb3rHg"
      },
      "source": [
        "The native format for this data is a .zip file, and we want the .shp file within the .zip file, so we must first extract the data to open it. First, we'll programmatically get the filename we just downloaded, and then extract all data to the `data_downloads` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed_mTohH3rHg"
      },
      "outputs": [],
      "source": [
        "filename = earthaccess.results.DataGranule.data_links(river_results[3], access='external')\n",
        "filename = filename[0].split(\"/\")[-1]\n",
        "filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW2hQ-CF3rHg"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(f'data_downloads/{filename}', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data_downloads')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE5Ge14q3rHh"
      },
      "source": [
        "Open the shapefile using `geopandas`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6RwizoQ3rHh"
      },
      "outputs": [],
      "source": [
        "filename_shp = filename.replace('.zip','.shp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5dZOp-O3rHh"
      },
      "outputs": [],
      "source": [
        "SWOT_HR_shp1 = gpd.read_file(f'data_downloads/{filename_shp}')\n",
        "\n",
        "#view the attribute table\n",
        "SWOT_HR_shp1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJHd9KST3rHh"
      },
      "source": [
        "#### Quickly plot the SWOT river data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY44pqbU3rHh"
      },
      "outputs": [],
      "source": [
        "# Simple plot\n",
        "fig, ax = plt.subplots(figsize=(10,7))\n",
        "SWOT_HR_shp1.plot(ax=ax, color='black')\n",
        "cx.add_basemap(ax, crs=SWOT_HR_shp1.crs, source=cx.providers.OpenTopoMap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DndxI6sA3rHh"
      },
      "outputs": [],
      "source": [
        "# Another way to plot geopandas dataframes is with `explore`, which also plots a basemap\n",
        "#SWOT_HR_shp1.explore()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "803Ughb13rHh"
      },
      "source": [
        "#### **2. Lake Vector Shapefiles**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhrHLXfL3rHh"
      },
      "source": [
        "The lake vector shapefiles can be accessed in the same way as the river shapefiles above.\n",
        "\n",
        "For additional tips on spatial searching of SWOT HR L2 data, see also [PO.DAAC Cookbook - SWOT Chapter tips section](https://podaac.github.io/tutorials/quarto_text/SWOT.html#tips-for-swot-hr-spatial-search)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTdxr0-43rHh"
      },
      "source": [
        "#### Search for data of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlHMg6UO3rHh"
      },
      "outputs": [],
      "source": [
        "lake_results = earthaccess.search_data(short_name = 'SWOT_L2_HR_LAKESP_2.0',\n",
        "                                        #temporal = ('2024-02-01 00:00:00', '2024-02-29 23:59:59'), # can also specify by time\n",
        "                                        granule_name = '*Obs*_535_NA*') # here we filter by files with 'Obs' in the name (This collection has three options: Obs, Unassigned, and Prior), by pass #535 (Halifax region) and by continent code=NA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJYhv_hb__7M"
      },
      "outputs": [],
      "source": [
        "#Print the granule characteristics associated with the selected pass.\n",
        "print(lake_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uay3D0DS3rHi"
      },
      "source": [
        "Let's download the selected data file! For the formation purposes, let's download a granule from the Halifax region. earthaccess.download has a list as the input format, so we need to put brackets around the single file we pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WvT4rEh3rHi"
      },
      "outputs": [],
      "source": [
        "earthaccess.download([lake_results[0]], \"./data_downloads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "barmJ-eQ3rHi"
      },
      "source": [
        "The native format for this data is a .zip file, and we want the .shp file within the .zip file, so we must first extract the data to open it. First, we'll programmatically get the filename we just downloaded, and then extract all data to the `SWOT_downloads` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uHEgBWL3rHi"
      },
      "outputs": [],
      "source": [
        "filename2 = earthaccess.results.DataGranule.data_links(lake_results[0], access='external')\n",
        "filename2 = filename2[0].split(\"/\")[-1]\n",
        "filename2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL9U2jls3rHi"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(f'data_downloads/{filename2}', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data_downloads')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx4N4fCr3rHi"
      },
      "source": [
        "Open the shapefile using `geopandas`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB0mMazr3rHi"
      },
      "outputs": [],
      "source": [
        "filename_shp2 = filename2.replace('.zip','.shp')\n",
        "filename_shp2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T03QYmUd3rHi"
      },
      "outputs": [],
      "source": [
        "SWOT_HR_shp2 = gpd.read_file(f'data_downloads/{filename_shp2}')\n",
        "\n",
        "#view the attribute table\n",
        "SWOT_HR_shp2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbPu8_GA3rHi"
      },
      "source": [
        "#### Quickly plot the SWOT lakes data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZNXoGEI3rHi"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "SWOT_HR_shp2.plot(ax=ax, color='black')\n",
        "ax.set_ylim(44,47) #select your area of interest to zoom in\n",
        "ax.set_xlim(-64,-62) #select your area of interest to zoom in\n",
        "cx.add_basemap(ax, crs=SWOT_HR_shp2.crs, source=cx.providers.OpenTopoMap)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8FCTuFj3rHi"
      },
      "source": [
        "Accessing the remaining files is different than the shp files above. We do not need to extract the shapefiles from a zip file because the following SWOT HR collections are stored in **netCDF** files in the cloud. For the rest of the products, we will open via `xarray`, not `geopandas`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5VVWM3T3rHj"
      },
      "source": [
        "#### **3. Water Mask Pixel Cloud NetCDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyzubIWy3rHj"
      },
      "source": [
        "#### Search for data collection and time of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI7j-xMH3rHj"
      },
      "outputs": [],
      "source": [
        "pixc_results = earthaccess.search_data(short_name = 'SWOT_L2_HR_PIXC_2.0',\n",
        "                                        #temporal = ('2024-02-01 00:00:00', '2024-02-29 23:59:59'), # can also specify by time\n",
        "                                        #granule_name = '*_535_233L*'), # pass number 535, tile number 233, left swath (Halifax region)\n",
        "                                        bounding_box = (-63.83,44.82,-63.36,44.99)) # filter by bouding box EX : Shubenacadie Grand Lake near Halifax, to find your bounding box : http://bboxfinder.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uo-3hTY45bdY"
      },
      "outputs": [],
      "source": [
        "#Print the granule characteristics associated with the selected pass, tile and swath.\n",
        "print(pixc_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT6c7eod3rHj"
      },
      "source": [
        "Let's download one data file! earthaccess.download has a list as the input format, so we need to put brackets around the single file we pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ou7k7iha3rHj"
      },
      "outputs": [],
      "source": [
        "earthaccess.download([pixc_results[0]], \"./data_downloads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQE3zVRR3rHm"
      },
      "source": [
        "#### Open data using xarray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gPsWBV_3rHm"
      },
      "source": [
        "The pixel cloud netCDF files are formatted with three groups titled, \"pixel cloud\", \"tvp\", or \"noise\" (more detail [here](https://podaac-tools.jpl.nasa.gov/drive/files/misc/web/misc/swot_mission_docs/pdd/D-56411_SWOT_Product_Description_L2_HR_PIXC_20200810.pdf)). In order to access the coordinates and variables within the file, a group must be specified when calling xarray open_dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKmO1lj63rHm"
      },
      "outputs": [],
      "source": [
        "ds_PIXC = xr.open_mfdataset(\"data_downloads/SWOT_L2_HR_PIXC_*.nc\", group = 'pixel_cloud', engine='h5netcdf') #If several PIXC files are uploaded to the data_downlaod file, specify which of the files to display\n",
        "ds_PIXC\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM7lzUnr3rHm"
      },
      "source": [
        "#### Simple plot of the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOs0fhyv3rHm"
      },
      "outputs": [],
      "source": [
        "# This could take a few minutes to plot\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "cax=plt.scatter(x=ds_PIXC.longitude, y=ds_PIXC.latitude, c=ds_PIXC.height, s=1) #adjust pixel size with s= desired size\n",
        "cbar = fig.colorbar(cax, ax=ax, shrink=0.5)\n",
        "cbar.set_label('Height (m)')\n",
        "\n",
        "# adjust colobar\n",
        "cbar.ax.set_aspect('auto')\n",
        "\n",
        "cx.add_basemap(ax, crs='EPSG:4326', source=cx.providers.OpenTopoMap)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert PIXC into a shapefile to open in QGIS"
      ],
      "metadata": {
        "id": "iR-NrXv3p859"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from shapely.geometry import Point, Polygon\n",
        "\n",
        "lat_min = 45.2\n",
        "lat_max = 45.4\n",
        "lon_min = -63.7\n",
        "lon_max = -63.4\n",
        "\n",
        "lat = np.asarray(ds_PIXC.latitude[:])\n",
        "lon = np.asarray(ds_PIXC.longitude[:])\n",
        "\n",
        "\n",
        "mask = (lat > lat_min) & (lat < lat_max) & (lon > lon_min) & (lon < lon_max)\n",
        "\n",
        "data = {}\n",
        "exclude_vars = ['interferogram', 'illumination_time','illumination_time_tai','pixc_line_qual', 'pixc_line_to_tvp','data_window_first_valid','data_window_last_valid','data_window_first_cross_track','data_window_last_cross_track']\n",
        "for var_name in ds_PIXC.variables:\n",
        "    if var_name not in exclude_vars:\n",
        "        var_data = np.asarray(ds_PIXC.variables[var_name][:])\n",
        "        var_data = var_data[mask]\n",
        "        if len(var_data.shape) > 1:\n",
        "            var_data = var_data.reshape(-1)\n",
        "        data[var_name] = var_data\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "points = [Point(x, y) for x, y in zip(df.longitude, df.latitude)]\n",
        "\n",
        "gdf_out = gpd.GeoDataFrame(df, geometry=points, crs=\"EPSG:4326\")\n",
        "out_shpname = './data_downloads/SWOT_L2_HR_PIXC_001_186_075L_20230727T202350_20230727T202401_PGC0_01.shp'\n",
        "gdf_out.to_file(out_shpname)\n"
      ],
      "metadata": {
        "id": "9VoGLsbpdtxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxH9oIeDl-3P"
      },
      "outputs": [],
      "source": [
        "# Plot your masked PIXC data . You can also download the Shapefile and visualized it in QGIS\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "cax= plt.scatter(x=gdf_out.longitude, y=gdf_out.latitude, c=gdf_out.height, s=1) #adjust pixel size with s= desired size\n",
        "cbar = fig.colorbar(cax, ax=ax, shrink=0.5)\n",
        "cbar.set_label('Height (m)')\n",
        "\n",
        "# adjust colobar\n",
        "cbar.ax.set_aspect('auto')\n",
        "\n",
        "cx.add_basemap(ax, crs='EPSG:4326', source=cx.providers.OpenTopoMap)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulakBaLH3rHm"
      },
      "source": [
        "#### **4. Water Mask Pixel Cloud Vector Attribute NetCDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkTelbKM3rHm"
      },
      "source": [
        "#### Search for data of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqAU4RuT3rHn"
      },
      "outputs": [],
      "source": [
        "pixcvec_results = earthaccess.search_data(short_name = 'SWOT_L2_HR_PIXCVEC_2.0',\n",
        "                                        #temporal = ('2024-02-01 00:00:00', '2024-02-29 23:59:59'), # can also specify by time\n",
        "                                        #granule_name = '*_535_233L*'), # pass number 535, tile number 233, left swath\n",
        "                                        bounding_box = (-63.83,44.82,-63.36,44.99)) # filter by bouding box EX : Shubenacadie Grand Lake near Halifax, to find your bounding box : http://bboxfinder.com/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26uTWPEj7CMv"
      },
      "outputs": [],
      "source": [
        "#Print the granule characteristics.\n",
        "print(pixcvec_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkwvQI1i3rHn"
      },
      "source": [
        "Let's download the first data file! earthaccess.download has a list as the input format, so we need to put brackets around the single file we pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SipHO_Lh3rHn"
      },
      "outputs": [],
      "source": [
        "earthaccess.download([pixcvec_results[0]], \"./data_downloads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj1MBbdx3rHn"
      },
      "source": [
        "#### Open data using xarray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkufSBay3rHn"
      },
      "source": [
        "First, we'll programmatically get the filename we just downloaded and then view the file via `xarray`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h40-TZA13rHn"
      },
      "outputs": [],
      "source": [
        "ds_PIXCVEC = xr.open_mfdataset(\"data_downloads/SWOT_L2_HR_PIXCVec_*.nc\", decode_cf=False,  engine='h5netcdf')\n",
        "ds_PIXCVEC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btcux1t93rHn"
      },
      "source": [
        "#### Simple plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZWenCYj3rHn"
      },
      "outputs": [],
      "source": [
        "pixcvec_htvals = ds_PIXCVEC.height_vectorproc.compute()\n",
        "pixcvec_latvals = ds_PIXCVEC.latitude_vectorproc.compute()\n",
        "pixcvec_lonvals = ds_PIXCVEC.longitude_vectorproc.compute()\n",
        "\n",
        "#Before plotting, we set all fill values to nan so that the graph shows up better spatially\n",
        "pixcvec_htvals[pixcvec_htvals > 15000] = np.nan\n",
        "pixcvec_latvals[pixcvec_latvals < 1] = np.nan\n",
        "pixcvec_lonvals[pixcvec_lonvals > -1] = np.nan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtKXWIxM3rHn"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "plt.scatter(x=pixcvec_lonvals, y=pixcvec_latvals, c=pixcvec_htvals, s=1) #adjust pixel size with s= desired size\n",
        "plt.colorbar().set_label('Height (m)')\n",
        "\n",
        "cx.add_basemap(ax, crs='EPSG:4326', source=cx.providers.OpenTopoMap)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfY8ht273rHn"
      },
      "source": [
        "#### **5. Raster NetCDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51PSnIrO3rHo"
      },
      "source": [
        "#### Search for data of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6EWLsgY3rHo"
      },
      "outputs": [],
      "source": [
        "raster_results = earthaccess.search_data(short_name = 'SWOT_L2_HR_Raster_2.0',\n",
        "                                        #temporal = ('2024-02-01 00:00:00', '2024-02-29 23:59:59'), # can also specify by time\n",
        "                                        #bounding_box = (-63.83,44.82,-63.36,44.99), # filter by bouding box EX : Shubenacadie Grand Lake near Halifax, to find your bounding box : http://bboxfinder.com/\n",
        "                                        granule_name = '*100m*535_117*') # here we filter by files with '100m' in the name (This collection has two resolution options: 100m & 250m), pass 535 and scene 117\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B43pLnhHC5z"
      },
      "outputs": [],
      "source": [
        "#Print the granule characteristics associated with the selected pass, scene and resolution.\n",
        "print(raster_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCOLktF73rHo"
      },
      "source": [
        "Let's download one data file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgL6WGbE3rHo"
      },
      "outputs": [],
      "source": [
        "earthaccess.download([raster_results[0]], \"./data_downloads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuvxbYkj3rHo"
      },
      "source": [
        "#### Open data with xarray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mm9T2Zv3rHo"
      },
      "source": [
        "First, we'll programmatically get the filename we just downloaded and then view the file via `xarray`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsOMnsqW3rHo"
      },
      "outputs": [],
      "source": [
        "ds_raster = xr.open_mfdataset(f'data_downloads/SWOT_L2_HR_Raster*', engine='h5netcdf')\n",
        "ds_raster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIoYfcIt3rHo"
      },
      "source": [
        "#### Quick interactive plot with `hvplot`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USz9iQ493rHo"
      },
      "outputs": [],
      "source": [
        "hv.extension('bokeh', 'matplotlib')\n",
        "plot = ds_raster['wse'].hvplot.image(y='y', x='x')\n",
        "hv.output(plot)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Masking a variable with its quality flag\n",
        "Example for L2_HR_Raster, flag \"wse_qual\":\\\n",
        "0 = good\\\n",
        "1 = suspect - may have large errors\\\n",
        "2 = degraded - likely to have large errors\\\n",
        "3 = bad - may be nonsensical and should be ignored"
      ],
      "metadata": {
        "id": "xlNpHaxlPFXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "variable_to_mask = ds_raster['wse']\n",
        "mask_variable = ds_raster['wse_qual']\n"
      ],
      "metadata": {
        "id": "JsVbk_B-RpZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the condition for masking based on the range of the quality flag\n",
        "mask_condition = mask_variable <2\n",
        "masked_variable = variable_to_mask.where(mask_condition)\n",
        "masked_variable\n"
      ],
      "metadata": {
        "id": "VCX8XB1RV5rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the masked variable in the dataset\n",
        "hv.extension('bokeh', 'matplotlib')\n",
        "plot2 = masked_variable.hvplot.image(y='y', x='x')\n",
        "hv.output(plot2)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uFJ0Gf1TWpWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geoviews"
      ],
      "metadata": {
        "id": "D50gpS-uqD6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXVIrdGW3rHo"
      },
      "source": [
        "#### **6. SLC**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQQGZxem3rHo"
      },
      "source": [
        "#### Search for data collection and time of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkcA1aRK3rHp"
      },
      "outputs": [],
      "source": [
        "slc_results = earthaccess.search_data(short_name = 'SWOT_L1B_HR_SLC_2.0',\n",
        "                                        #temporal = ('2024-01-01 00:00:00', '2024-01-30 23:59:59'), # can also specify by time\n",
        "                                        #bounding_box = (-63.83,44.82,-63.36,44.99), # filter by bouding box EX : Shubenacadie Grand Lake near Halifax, to find your bounding box : http://bboxfinder.com/\n",
        "                                        granule_name = '*_535_233L*') # pass number 535, tile number 233, left swath (Halifax region)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONevXJVO3rHp"
      },
      "source": [
        "Let's download one data file! earthaccess.download has a list as the input format, so we need to put brackets around the single file we pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKdm-CVI3rHp"
      },
      "outputs": [],
      "source": [
        "earthaccess.download([slc_results[0]], \"./data_downloads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWmkl6de3rHp"
      },
      "source": [
        "#### Open data using xarray\n",
        "The L1B_HR_SLC product file contains five NetCDF data group called the slc, xfactor, noise, tvp, and grdem groups. More info can be found in the [product description document within the dataset table](https://podaac.jpl.nasa.gov/SWOT?tab=datasets) for each group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V0OXaFk3rHp"
      },
      "outputs": [],
      "source": [
        "ds_SLC = xr.open_mfdataset(\"data_downloads/SWOT_L1B_HR_SLC*.nc\", group = 'slc', engine='h5netcdf')\n",
        "ds_SLC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU0-gvHiaob9"
      },
      "source": [
        "#### **7. Clip your data before download**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn9JzF4DcCB3"
      },
      "outputs": [],
      "source": [
        "# Create a file for your clipped data\n",
        "os.makedirs('clip_data', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vC2rOSW4oYDB"
      },
      "outputs": [],
      "source": [
        "#Load the shapefile of your region of interest\n",
        "#You can find the Shubencadie Lake data here: https://drive.google.com/drive/folders/1fliVPuqtsK2mvY-64h3KNKQ-4NO-rZxX?usp=sharing\n",
        "!gdown --id 15M4KDl7bF96pFD1VewrYRqBeq_JJSKKM -O /content/Shubenacadie_Grand_Lake.zip\n",
        "!unzip -o /content/Shubenacadie_Grand_Lake.zip\n",
        "ROI_shapefile_path = '/content/Shubenacadie_Grand_Lake/Shubenacadie_Grand_Lake.shp' #drop your shapefile in the folder section in the left panel. You have to drop all files in the folder\n",
        "ROI = gpd.read_file(ROI_shapefile_path,crs=\"epsg:4326\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdT0uGzDawbS"
      },
      "source": [
        "#### Clip your shapefile data (RiverSP or LakeSP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xp66LOtYbAoR"
      },
      "outputs": [],
      "source": [
        "# Load the shapefile to be clipped\n",
        "#shapefile_path = '/content/data_downloads/SWOT_L2_HR_LakeSP_Obs_010_535_NA_20240213T024323_20240213T025411_PIC0_01.shp'\n",
        "shapefile_path = '/content/data_downloads/SWOT_L2_HR_LakeSP_Obs_003_535_NA_20230920T012750_20230920T013838_PGC0_01.shp'\n",
        "gdf = gpd.read_file(shapefile_path)\n",
        "\n",
        "# Clip the shapefile\n",
        "clipped_gdf = gpd.clip(gdf, ROI)\n",
        "\n",
        "# Save the clipped shapefile\n",
        "output_path = '/content/clip_data/clipped_shapefile.shp'\n",
        "clipped_gdf.to_file(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDn9o1htnTaB"
      },
      "source": [
        "#### Clip your Raster NetCDF data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFX2Yy1-nZKY"
      },
      "outputs": [],
      "source": [
        "# Enter the path of your data\n",
        "data = xr.open_dataset('/content/data_downloads/SWOT_L2_HR_Raster_100m_UTM20T_N_x_x_x_010_535_117F_20240213T025105_20240213T025121_PIC0_01.nc')\n",
        "\n",
        "#Select the variable of your choice\n",
        "var_data = data['wse']\n",
        "\n",
        "# Set the dataset’s spatial dimensions and the coordinate reference system(crs).\n",
        "var_data.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=True)\n",
        "var_data.rio.write_crs(\"epsg:32620\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiLOrYStn7MS"
      },
      "outputs": [],
      "source": [
        "# If necessary, reproject the ROI shapefile to the same EPSG as the netCDF one\n",
        "ROI = ROI.to_crs(\"epsg:32620\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qkm1TbdoKzR"
      },
      "outputs": [],
      "source": [
        "# Clip the raster\n",
        "clipped = var_data.rio.clip(ROI.geometry.apply(mapping), ROI.crs, drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Otx9IbRsiD7"
      },
      "outputs": [],
      "source": [
        "# Remove the 'grid_mapping' attribute\n",
        "if 'grid_mapping' in clipped.attrs:\n",
        "    del clipped.attrs['grid_mapping']\n",
        "\n",
        "# Save the clipped raster to a NetCDF file\n",
        "clipped_path = '/content/clip_data/clipped_raster.nc'  # Set the path and the name of the file to save the clipped NetCDF\n",
        "clipped.to_netcdf(clipped_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu09EYYRxxd4"
      },
      "source": [
        "#### **8. Transformation of reference systems**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY6Xlk9_yl1o"
      },
      "source": [
        "The system of reference of SWOT is not the same that the one of canada. There is a need to convert the data.\n",
        "**Warning : Each Canadian provincial geodetic agency can adopt a different vertical reference system and epoch!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLhZa39hyYkh"
      },
      "source": [
        "Create a directory and automatically download the mask shapefile corresponding to your province\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUat6UDKyTAp"
      },
      "outputs": [],
      "source": [
        "# Create a file for your data extraction\n",
        "import os\n",
        "os.makedirs('ref_change', exist_ok=True)\n",
        "\n",
        "!wget -P '/content/ref_change' 'https://nsgi.novascotia.ca/WSF_DDS/DDS.svc/DownloadFile?tkey=kNNpTdP4QuNRSYtt&id=26201'\n",
        "\n",
        "import zipfile\n",
        "destination_dir = '/content/ref_change'\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "file_path = '/content/ref_change/DownloadFile?tkey=kNNpTdP4QuNRSYtt&id=26201'\n",
        "zip_path = '/content/ref_change/downloaded_file.zip'\n",
        "\n",
        "# Renommer le fichier pour lui donner une extension .zip\n",
        "os.rename(file_path, zip_path)\n",
        "\n",
        "# Vérifier si le fichier est un zip valide\n",
        "is_zip_valid = zipfile.is_zipfile(zip_path)\n",
        "print(f\"Le fichier téléchargé est un zip valide: {is_zip_valid}\")\n",
        "\n",
        "if is_zip_valid:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(destination_dir)\n",
        "    print(f\"Le fichier zip a été extrait avec succès dans le répertoire : {destination_dir}\")\n",
        "else:\n",
        "    print(\"Le fichier téléchargé n'est pas un zip valide.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ruy3WM3zaOa"
      },
      "source": [
        "Clip the data with a Nova Scotia mask (needed because each province can adopt a different epoch and vertical reference system)\n",
        "\n",
        "Warning : change the name of the RiverSp file to match the one you want\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjbUIcpx1Xg1"
      },
      "outputs": [],
      "source": [
        "#\n",
        "from shapely.ops import unary_union\n",
        "\n",
        "#Load the shapefile of Nova Scotia borders\n",
        "mask_shapefile_path = '/content/ref_change/County_Polygons.shp'\n",
        "mask_gpd = gpd.read_file(mask_shapefile_path)\n",
        "\n",
        "#Merge Multypolygons\n",
        "merged_gpd = mask_gpd.unary_union\n",
        "mask = gpd.GeoDataFrame([1], geometry=[merged_gpd], crs=mask_gpd.crs)\n",
        "\n",
        "# Load the shapefile to be clipped and verify that the mask and the RiverSP file have the same coordinate system\n",
        "shapefile_path = '/content/data_downloads/SWOT_L2_HR_RiverSP_Node_013_492_NA_20240414T034216_20240414T034217_PIC0_01.shp'\n",
        "gdf = gpd.read_file(shapefile_path)\n",
        "\n",
        "mask = mask.to_crs(crs=gdf.crs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtUKZakh2j1D"
      },
      "outputs": [],
      "source": [
        "# Clip the shapefile\n",
        "clipped_gdf = gpd.clip(gdf, mask)\n",
        "\n",
        "# Save the clipped shapefile\n",
        "output_path = '/content/ref_change/Nova_scotia_clipped.shp'\n",
        "clipped_gdf.to_file(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot your cropped data"
      ],
      "metadata": {
        "id": "TkDhwpMKz4fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple plot\n",
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "clipped_gdf.plot(ax=ax, color='black')\n",
        "mask.boundary.plot(ax=ax, edgecolor='red')\n",
        "cx.add_basemap(ax, crs=gdf.crs, source=cx.providers.OpenTopoMap)"
      ],
      "metadata": {
        "id": "sjh5n79Yz8jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-VqZF6sy3Fq"
      },
      "source": [
        "Then, the data must be extracted and saved in the rigth format to use it with the TRX tool from NRCan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HX9hweny8V1Z"
      },
      "outputs": [],
      "source": [
        "# Enter the path of your data\n",
        "data_all = []\n",
        "\n",
        "with shapefile.Reader('/content/ref_change/Nova_scotia_clipped.shp') as shp:\n",
        "\n",
        "    fields = [field[0] for field in shp.fields if field[0] != 'DeletionFlag']\n",
        "\n",
        "    for record in shp.records():\n",
        "          attributs = dict(zip(fields, record))\n",
        "          #Select the node_id, latitude, longitude and wse that arenot -999999999999\n",
        "          if all(value != -999999999999 for value in attributs.values()):\n",
        "            data_all.append([attributs['node_id'],attributs['lat'],attributs['lon'],attributs['wse']])\n",
        "\n",
        "header = ['Station','latitude','longitude','height']\n",
        "\n",
        "# Creation and writing CSV file\n",
        "with open('/content/ref_change/wse_nova_scotia.csv', 'w', newline='', encoding='utf-8') as fichier:\n",
        "    writer = csv.writer(fichier)\n",
        "    writer.writerow(header)\n",
        "\n",
        "    # Data writing\n",
        "    for data in data_all:\n",
        "        writer.writerow(data)\n",
        "\n",
        "#Save on the computer\n",
        "from google.colab import files\n",
        "files.download(\"/content/ref_change/wse_nova_scotia.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7rjhEwHzX-3"
      },
      "source": [
        "Finally, to use TRX online, go to the website : https://webapp.csrs-scrs.nrcan-rncan.gc.ca/geod/tools-outils/trx.php?locale=en\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xMIm4Rgy9xx"
      },
      "source": [
        "Fill this informations in the **Batch proccesing** window:\n",
        "\n",
        "**Batch proccesing**\n",
        "\n",
        "**Origin** : Reference Frame : ITRF2014 epoque 01/01/2010\n",
        "\n",
        "**Destination** : Reference Frame : NAD83(CSRS) Coordinates : Geographic\n",
        "\n",
        " **Epoch Transformation** : 01/01/2010 (adopted epoch for Nova Scotia, adapt the epoch accordingly)\n",
        "\n",
        "Select the file downloaded previously."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Munla4LN81su"
      },
      "source": [
        "#### **9. Download all your SWOT data to your local computer at once**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF1okpJjGAny"
      },
      "source": [
        "You can download data one by one by clicking on the three small dots to the right of the file name and then on “Download”. To download all the data at once, run the following cells. Execution and downlaod times may be long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7fiZ1tk-uSZ"
      },
      "outputs": [],
      "source": [
        "# Create a zip file with all data and choose the file to compress\n",
        "!zip -r /content/data.zip /content/data_downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04WSzmVY-5gz"
      },
      "outputs": [],
      "source": [
        "# Download data zip file\n",
        "from google.colab import files\n",
        "files.download(\"/content/data.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **10. Working with HYDROCON**\n"
      ],
      "metadata": {
        "id": "iNw1ksdCoNu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract Hydrocon information and plot Reaches or nodes of interest"
      ],
      "metadata": {
        "id": "_lP3hh9GY6WI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import And\n",
        "import folium\n",
        "import requests\n",
        "from io import StringIO\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# You can choose a node id from the RiverSP product obtained in section 1\n",
        "#Example for Saint John River\n",
        "\n",
        "#feature='Node'\n",
        "#feature_id=\"72608100300075\"\n",
        "feature=\"Reach\"\n",
        "feature_id=\"72608100305\"\n",
        "#start_time=\"2024-01-01T00:00:00Z\"\n",
        "#end_time=\"2024-06-06T00:00:00Z\"\n",
        "#fields=reach_id,time_str,wse,width\n",
        "\n",
        "parameters = \"https://soto.podaac.earthdatacloud.nasa.gov/hydrocron/v1/timeseries?feature=\"+feature+\"&feature_id=\"+ feature_id +\"&start_time=2024-01-01T00:00:00Z&end_time=2024-06-06T00:00:00Z&output=geojson&fields=reach_id,time_str,wse,width,cycle_id\"\n",
        "\n",
        "hydrocron_response = requests.get(\n",
        "    parameters\n",
        ").json()\n",
        "\n",
        "hydrocron_response\n",
        "\n",
        "# extract just the geojson to plot on the map\n",
        "\n",
        "geojson_data = hydrocron_response['results']['geojson']\n",
        "\n",
        "geojson_data\n",
        "\n",
        "# Set up the map using Folium (https://python-visualization.github.io/folium/latest/)\n",
        "\n",
        "map = folium.Map (zoom_start=13, tiles=\"cartodbpositron\", width=700, height=700)\n",
        "\n",
        "# add the geojson from Hydrocron to the map\n",
        "folium.GeoJson(geojson_data, name='SWOT River Reach').add_to(map)\n",
        "folium.LayerControl().add_to(map)\n",
        "\n",
        "# zoom to the river feature we added\n",
        "map.fit_bounds(map.get_bounds(), padding=(5, 5))\n",
        "\n",
        "map\n"
      ],
      "metadata": {
        "id": "Kx2hHQkCoMvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot time series of WSE for a chosen Node"
      ],
      "metadata": {
        "id": "4HPkZ9hgZJtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example for Saint John River\n",
        "\n",
        "feature='Node'\n",
        "feature_id=\"72608100300075\"\n",
        "#start_time=\"2024-01-01T00:00:00Z\"\n",
        "#end_time=\"2024-06-06T00:00:00Z\"\n",
        "#fields=reach_id,time_str,wse,width\n",
        "\n",
        "parameters = \"https://soto.podaac.earthdatacloud.nasa.gov/hydrocron/v1/timeseries?feature=\"+feature+\"&feature_id=\"+ feature_id +\"&start_time=2024-01-01T00:00:00Z&end_time=2024-06-06T00:00:00Z&output=csv&fields=reach_id,node_id,time_str,node_q,wse,width,cycle_id\"\n",
        "\n",
        "hydrocron_response = requests.get(\n",
        "    parameters\n",
        ").json()\n",
        "\n",
        "hydrocron_response\n",
        "csv_str = hydrocron_response['results']['csv']\n",
        "df = pd.read_csv(StringIO(csv_str))\n",
        "ind = df.node_q<3\n",
        "\n",
        "df = df[df['time_str'] != 'no_data']\n",
        "df.time_str = pd.to_datetime(df.time_str, format='%Y-%m-%dT%H:%M:%SZ')\n",
        "fig = plt.figure(figsize=(15,5))\n",
        "plt.plot(df.time_str[ind], df.wse[ind], marker='o')\n",
        "\n",
        "plt.ylabel('Water surface elevation (m)')\n",
        "plt.xlabel('SWOT observation date')\n",
        "plt.title('Water Surface Elevation from Hydrocron for Node: ' + str(df.node_id[0]))\n"
      ],
      "metadata": {
        "id": "KJdeAj8RZOzQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "5a4443810289f87e0f862ef34d31d94a0884467de587e41820bef73e0713c5c1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}